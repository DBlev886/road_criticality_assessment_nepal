# -*- coding: utf-8 -*-
"""
Created on Sun May  8 12:04:30 2022

@author: Danny Baldig
title: "Automating criticality assessments of rural road networks in Nepal" 

Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
venv\scripts\activate
"""

#%%
########################################################################################
################################## INPUTS AND IMPORTS ##################################
########################################################################################
download_path ='C:/Users/Danny/Desktop/EMMA/IV/Data/' # enter path for (temporary) downloads
lc_path = "C:/Users/Danny/Desktop/EMMA/IV/Data/Land_cover/data/LandCover_NP_2019.tif" # path of land cover raster; trying to automate this later but download is behind log-in wall, so that requires an account for every user
study_area = (["Beni, Nepal", "Jaljala, Nepal"])    # Use OSM Nominatim in comma-seperated strings: https://nominatim.openstreetmap.org/ui/search.html
population_type = "" # ("constrained": for official population counts or "UNadj_constrained" for UN adjusted population counts)

import osmnx as ox
import networkx as nx
import geopandas as gpd
import shapely
import rasterio as rio
from rasterio.mask import mask
import rasterstats
from rasterio.warp import calculate_default_transform, reproject, Resampling
import pandas as pd
from pyproj import CRS
import numpy as np
from rasterstats import zonal_stats
from shapely import speedups
speedups.enabled
from shapely.geometry import Point, LineString
import pycountry
from wpgpDownload.utils.wpcsv import Product
import requests


#%%
pd.set_option('display.max_colwidth', None)


#%%
########################################################################################
############################ RETRIEVE AND PROCESS OSM-DATA #############################
########################################################################################

################################# FETCH WHOLE NETWORK ##################################
full_network = ox.graph_from_place(study_area,
                            network_type="drive",
                            clean_periphery=True)

# project the data
full_network_proj = ox.project_graph(full_network)
all_nodes, all_edges = ox.graph_to_gdfs(full_network_proj,
                            nodes=True,
                            edges=True)

'''edges = edges.drop(
                ["lanes",
                 "maxspeed",
                 "access"], 
                 axis=1)'''


#%%
################################# FETCH ONLY MAIN ROADS #################################
cf = '["highway"~"trunk|primary|tertiary"]'
main_network = ox.graph_from_place(study_area,
                        simplify=False, 
                        custom_filter=cf)
#fig, ax = ox.plot_graph(G)

main_network_proj = ox.project_graph(main_network)
main_nodes, main_edges = ox.graph_to_gdfs(main_network_proj,
                                nodes=True,
                                edges=True)

main_edges = main_edges[main_edges['reversed'].isin([0])]

#%%
trunk = main_edges[main_edges['highway'].isin(['trunk'])]
primary = main_edges[main_edges['highway'].isin(['primary'])]
tertiary = main_edges[main_edges['highway'].isin(['tertiary'])]


#%%
trunk_union = trunk.unary_union
trunk_union = shapely.ops.linemerge(trunk_union)
trunk_union = gpd.GeoDataFrame(index=[0], crs=all_nodes.crs, geometry=[trunk_union])

primary_union = primary.unary_union
primary_union = shapely.ops.linemerge(primary_union)
primary_union = gpd.GeoDataFrame(index=[0], crs=all_nodes.crs, geometry=[primary_union])

tertiary_union = tertiary.unary_union
tertiary_union = shapely.ops.linemerge(tertiary_union)
tertiary_union = gpd.GeoDataFrame(index=[0], crs=all_nodes.crs, geometry=[tertiary_union])


#%%
def cut(line, distance):
    # Cuts a line in two at a distance from its starting point
    if distance <= 0.0 or distance >= line.length:
        return [LineString(line)]
    coords = list(line.coords)
    for i, p in enumerate(coords):
        pd = line.project(Point(p))
        if pd == distance:
            return [
                    LineString(coords[:i+1]),
                    LineString(coords[i:])]
        if pd > distance:
            cp = line.interpolate(distance)
            return [
                    LineString(coords[:i] + [(cp.x, cp.y)]),
                    LineString([(cp.x, cp.y)] + coords[i:])]


def MultiCut(line, pieces):
    #Firts we need to define two list to append the results (lines) and the lines to cut
    lines_result = [] #All the lines are add to this list
    lines_to_cut = [] #The lines to cut are add to this list

    #We must ensure that pieces are higher than 2
    if pieces == 1: #If pieces are 1 the result is the same line
        lines_result.append(line) 
    elif pieces == 2: #If pieces are 2 the result is the line cut by the middle point
        distance = (line.length)/pieces
        lines_result.append(cut(line, distance)[0])
        lines_result.append(cut(line, distance)[1])
    else: # If pieces is more than 3 we star to cut the line in the number of pieces
        # We use a loop  from the first to the penultimate piece 
        for i in range(1, pieces): 
            # The first piece is cut to save the result and the rest of the line
            if i == 1:
                distance = (line.length)/pieces #Distance is calculate as the lenght of the line divided by the number of pieces
                lines_result.append(cut(line, distance)[0]) #We cut the line and save the first part in lines result
                lines_to_cut = cut(line, distance)[1] #We save the rest of the line in lines to cut in order to continue with the split

            #If pieces are equal to pieces minus two we can split the line and 
            #save only the first part in lines result in order to continue with 
            #the split
            if  1 < i <= pieces - 2:
                distance = (line.length)/pieces
                lines_result.append(cut(lines_to_cut, distance)[0])
                lines_to_cut = cut(lines_to_cut, distance)[1]

            #Finally if pieces are equal to pieces minus 1 we can cut the line 
            #and save both of the parts in lines result
            if (i != 1) and (i == pieces-1):
                distance = (line.length)/pieces
                lines_result.append(cut(lines_to_cut, distance)[0])
                lines_result.append(cut(lines_to_cut, distance)[1])

    return lines_result


#%%
#I took the lines (LineString) of the geodataframe

edge = trunk_union.reset_index()
network_to_cut = edge.geometry

results = []
for j in range(0, len(network_to_cut)):
    if network_to_cut[j].length > 1000:
        line = network_to_cut[j]
        pieces = int(((line.length)//1000)+1)
        results.append(list(MultiCut(line, pieces)))


#%%
distances = pd.DataFrame(results).transpose()
distances.columns = ['geom']
gdf = gpd.GeoDataFrame(distances, geometry='geom', crs=all_nodes.crs)


#%%
'''
def find_adjacent(line): # for finding adjacent features
    outlist = []
    outinds = []
    outset = set()
    for j, l in line.iteritems():
        as_set = set(line.astype(str))
        inds = []
        for k in outset.copy():
            if outlist[k] & as_set:
                outset.remove(k)
                as_set |= outlist[k]
                inds.extend(outinds[k])
        outset.add(j)
        outlist.append(as_set)
        outinds.append(inds + [j])
    outinds = [outinds[j] for j in outset]
    del outset, outlist
    result = [[line[j] for j in k] for k in outinds]
    return result

lines = find_adjacent(trunk.geometry)
'''


#%%
'''
already_processed = []
for feat in trunk:
    attrs = feat.attributes()
    geom = feat.geometry()
    curr_id = feat["highway"]
    if curr_id not in already_processed:
        query = '"highway" = %s' % (curr_id)
        selection = layer.getFeatures(QgsFeatureRequest().setFilterExpression(query))
        selected_ids = [k.geometry().asPolyline() for k in selection]
        adjacent_feats = find_adjacent(selected_ids)
        for f in adjacent_feats:
            first = True
            for x in xrange(0, len(f)):
                geom = (QgsGeometry.fromPolyline([QgsPoint(w) for w in f[x]]))
                if first:
                    outFeat = QgsFeature()
                    outFeat.setGeometry(geom)
                    outGeom = outFeat.geometry()
                    first = False
                else:
                    outGeom = outGeom.combine(geom)
            outFeat.setAttributes(attrs)
            outFeat.setGeometry(outGeom)
            prov.addFeatures([outFeat])
        already_processed.append(curr_id)
    else:
        continue
'''


#%%

########################################################################################
######################## ASSIGN STRATEGICAL IMPORTANCE SCORES ##########################
########################################################################################

###### ASSIGN CENTRALITY SCORES
### normalize centrality values to summarize them with population later to create quantiles
CC = nx.closeness_centrality(nx.line_graph(full_network_proj))
all_edges['cc'] = pd.DataFrame.from_dict(CC, orient='index')
all_edges['norm_cc'] = (all_edges['cc']-all_edges['cc'].min())/(all_edges['cc'].max()-all_edges['cc'].min())

BC = nx.betweenness_centrality(nx.line_graph(full_network_proj))#, weight="length")
all_edges['bc'] = pd.DataFrame.from_dict(BC, orient='index')
all_edges['norm_bc'] = (all_edges['bc']-all_edges['bc'].min())/(all_edges['bc'].max()-all_edges['bc'].min())

all_edges['centrality_avg'] = (all_edges['norm_cc'] + all_edges['norm_cc']) / 2


#%%
########################################################################################
################################## POPULATION RASTER ###################################
########################################################################################

# identify ISO country code to automatically download population raster based on OSM-input
if not download_path.endswith("/"):
    download_path = download_path + "/"

study_area_str = " ".join(study_area)
for country in pycountry.countries:
    if country.name in study_area_str:
        iso_code = country.alpha_3

#%%
products = Product(iso_code)  # Where instead of NPL it could be any valid ISO code.
#  to list all the products for NPL
#for p in products:
#    if "2020" in p.dataset_name: 
#        print('%s/%s\t%s\t%s' % (p.idx, p.country_name,p.dataset_name,p.path))

prod_name_input = "ppp_2020" + population_type

#dl(ISO=iso_code, out_folder=download_path, prod_name=prod_name_input)


#%%
# define boundaries to mask raster
### buffer distance crucial to connect roads on the outside and to allow connectivitiy between multiple administrative units
aoi = ox.geocode_to_gdf(study_area)
aoi = aoi.dissolve()
aoi = aoi.to_crs(CRS(all_nodes.crs))
aoi['geometry'] = aoi.geometry.buffer(2000)
aoi_file = download_path + "aoi.shp"
aoi.to_file(aoi_file)


#%%
# open downloaded raster, reproject, and mask with aoi boundary
# ERROR even though file is not opened somewhere else: CPLE_AppDefinedError: Deleting C:/Users/Danny/Desktop/EMMA/IV/Data/Population/npl_ppp_2020_UNadj_clipped.tif failed: Permission denied
raster_crs = CRS(aoi.crs).to_epsg()
mask_coords = aoi['geometry']
file_name = iso_code + "_" + prod_name_input + ".tif"
file_name = file_name.lower()
pop_path = download_path + file_name
file_substr = ".tif"
pop_idx = pop_path.index(file_substr)
pop_proj_tif = pop_path[:pop_idx] + "_reproj" + pop_path[pop_idx:]
pop_proj_clip_tif = pop_proj_tif[:pop_idx] + "_clipped" + pop_proj_tif[pop_idx:]

#%%
with rio.open(pop_path, mode='r+') as pop:
    transform, width, height = calculate_default_transform(
        pop.crs, raster_crs, pop.width, pop.height, *pop.bounds)
    kwargs = pop.meta.copy()
    kwargs.update({
        'crs': raster_crs,
        'transform': transform,
        'width': width,
        'height': height
    })

    with rio.open(pop_proj_tif, 'w', **kwargs) as pop_proj:
        for i in range(1, pop.count + 1):
            reproject(
                source=rio.band(pop, i),
                destination=rio.band(pop_proj, i),
                resampling=Resampling.nearest)


#%%
with rio.open(pop_proj_tif) as pop_proj:
    pop_out_image, out_transform = rio.mask.mask(pop_proj, aoi.geometry, crop=True)
    pop_out_meta = pop_proj.meta

pop_out_meta.update({"driver": "GTiff",
                 "height": pop_out_image.shape[1],
                 "width": pop_out_image.shape[2],
                 "transform": out_transform})

with rio.open(pop_proj_clip_tif, "w", **pop_out_meta) as pop_dest:
    pop_dest.write(pop_out_image)

#pop_dest.close() # close the rasterio dataset


#%%
# read clipped population raster and assign values to numpy nd array
#pop_count =  rio.open(clipped)
pop_raster = rio.open(pop_proj_clip_tif)
pop_count_array = pop_raster.read(1)
affine = pop_raster.transform


#%%
###################### BUFFER EDGES
# buffer edges
buffered = main_edges.buffer(100, 
                            cap_style=2,        # flat
                            join_style=2)       # mitre
buffered = gpd.GeoDataFrame(geometry=gpd.GeoSeries(buffered))
#buffered = buffered.dropna(axis=0, how=\"any\", thresh=None, subset=None, inplace=False)
#buffered.plot()


#%%

# calculating zonal statistics
pop_means = rasterstats.zonal_stats(gdf, pop_count_array, 
                                            affine = affine,
                                            nodata = np.nan,
                                            stats = ['mean'],
                                            geojson_out = True)

# extract average population data from list
pop_mean_list = []
i = 0

while i < len(pop_means):
    pop_mean_list.append(pop_means[i]['properties'])
    i = i + 1


#%%

# transfer information from list to DataFrame and assign scores based on quantiles ## get NaN values if not converting index
pop_mean = pd.DataFrame(pop_mean_list)
pop_mean = pop_mean.set_index(gdf.index)
gdf['pop_mean'] = pop_mean['mean']

#%%
cut_network = gpd.sjoin(gdf, all_edges, how="inner", predicate="overlaps")
#cut_network.rename(columns={"pop_mean_left": "pop_mean"})      doesn't work for whatever reason
cut_network['norm_pop'] = (cut_network['pop_mean_left']-cut_network['pop_mean_left'].min())/(cut_network['pop_mean_left'].max()-cut_network['pop_mean_left'].min())
cut_network['si_quantile'] = cut_network['centrality_avg'] + cut_network['norm_pop']


#%%%
# qcut: quantile-based discretization to assign scores from 1 to 5 to centrality values
cut_network['si_score'] = pd.qcut(cut_network['si_quantile'], q=5, labels=[1,2,3,4,5]).astype(str)

'''
edges.plot('norm_cc', label='centrality', cmap='RdYlBu_r', legend=True)
edges.plot('norm_pop', label='population', cmap='RdYlBu_r', legend=True)
edges.plot('si_score', label='strategic importance', cmap='RdYlBu_r', legend=True)
'''


#%%
########################################################################################
################################## LANDCOVER RASTER ####################################
########################################################################################

# trying to automatize land cover download /// doesn't work yet

site_url = 'http://rds.icimod.org/DatasetMasters/DownloadFile/19?metadataid=1972729'
userid = 'danny.baldig@student.uibk.ac.at'
password = 'icimodPW977'

file_url = 'http://rds.icimod.org/DatasetMasters/DownloadFile/19?metadataid=1972729'
o_file = 'nlcms_2019.zip'

# create session
s = requests.Session()
# GET request. This will generate cookie for you
s.get(site_url)
# login to site.
s.post(site_url, data={'_username': userid, '_password': password})
# Next thing will be to visit URL for file you would like to download.
r = s.get(file_url)

# Download file
with open(o_file, 'wb') as output:
    output.write(r.content)
print(f"requests:: File {o_file} downloaded successfully!")

# Close session once all work done
s.close()

#%%%

lc_idx = lc_path.index(file_substr)
lc_proj_tif = lc_path[:lc_idx] + "_reproj" + lc_path[lc_idx:]
lc_proj_clip_tif = lc_proj_tif[:lc_idx] + "_clipped" + lc_proj_tif[lc_idx:]

with rio.open(lc_path) as lc:
    transform, width, height = calculate_default_transform(
        lc.crs, raster_crs, lc.width, lc.height, *lc.bounds)
    kwargs = lc.meta.copy()
    kwargs.update({
        'crs': raster_crs,
        'transform': transform,
        'width': width,
        'height': height
    })

    with rio.open(lc_proj_tif, 'w', **kwargs) as lc_proj:
        for i in range(1, lc.count + 1):
            reproject(
                source=rio.band(lc, i),
                destination=rio.band(lc_proj, i),
                resampling=Resampling.nearest)

with rio.open(lc_proj_tif) as lc_proj:
    lc_out_image, out_transform = rio.mask.mask(lc_proj, mask_coords, crop=True)
    lc_out_meta = lc_proj.meta

lc_out_meta.update({"driver": "GTiff",
                 "height": lc_out_image.shape[1],
                 "width": lc_out_image.shape[2],
                 "transform": out_transform})

# output with wrong CRS again
with rio.open(lc_proj_clip_tif, "w", **lc_out_meta) as lc_dest:
    lc_dest.write(lc_out_image)
    lc_dest.close()


#%%
# get zonal statistics to get predominant land covers
cut_network['land_cover_unique'] = zonal_stats(cut_network, 
                                           lc_proj_clip_tif,
                                           categorical=True)


#%%
# land_cover codes: settlement: 6 + cropland: 7
# order important to assign higher value first
cut_network["land_cover_score"] = np.select([cut_network["land_cover_unique"].astype(str).str.contains(r"6:"), cut_network["land_cover_unique"].astype(str).str.contains(r"7:")], 
                                            [0.5, 0.3], 
                                            0)


#%%

########################################################################################
########################### FOOTPRINTS / OTHERS FOR LATER ##############################
########################################################################################

gdf = ox.geometries_from_place(study_area,
                               tags = {'building': True})
gdf_proj = ox.project_gdf(gdf)

gdf_centroid = gdf_proj.centroid
gdf_df = gpd.GeoDataFrame(geometry=gpd.GeoSeries(gdf_centroid))
west, south, east, north = aoi.unary_union.buffer(0.1).bounds


#%%
# Select a postal code area representing the city center of Helsinki

def intersect_using_spatial_index(source_gdf, intersecting_gdf):
    """
    Conduct spatial intersection using spatial index for candidates GeoDataFrame to make queries faster.
    Note, with this function, you can have multiple Polygons in the 'intersecting_gdf' and it will return all the points
    intersect with ANY of those geometries.
    """
    source_sindex = source_gdf.sindex
    possible_matches_index = []

    # 'itertuples()' function is a faster version of 'iterrows()'
    for other in intersecting_gdf.itertuples():
        bounds = other.geometry.bounds
        c = list(source_sindex.intersection(bounds))
        possible_matches_index += c

    # Get unique candidates
    unique_candidate_matches = list(set(possible_matches_index))
    possible_matches = source_gdf.iloc[unique_candidate_matches]

    # Conduct the actual intersect
    result = possible_matches.loc[possible_matches.intersects(intersecting_gdf.unary_union)]
    return result

#%%
# Test the spatial query with spatial index
list = intersect_using_spatial_index(source_gdf=gdf_centroid, intersecting_gdf=buffered)








# generate spatial index
sindex = buffered.sindex

#%%
# define empty list for results
results_list = []
# iterate over the points
for index, row in gdf_df.iterrows():
    buffer = row['geometry'].buffer(5)  # buffer
    # find approximate matches with r-tree, then precise matches from those approximate ones
    possible_matches_index = list(sindex.intersection(buffer.bounds))
    possible_matches = buffered.iloc[possible_matches_index]
    precise_matches = possible_matches[possible_matches.intersects(buffer)]
    results_list.append(len(precise_matches))
# add list of results as a new column
gdf_df['polygons'] = pd.Series(results_list)


#%%

'''

fig, ax = plt.subplots(figsize=(12,8))
# Plot edges and nodes
edges.plot(ax=ax, linewidth=0.75, color='gray')
nodes.plot(ax=ax, markersize=2, color='gray')
# Add buildings
ax = buildings.plot(ax=ax, facecolor='red', alpha=0.7)
# Add basemap
ctx.add_basemap(ax, crs=buildings.crs, source=ctx.providers.CartoDB.Positron)


def intersect_using_spatial_index(building_centroids, buffered):
    """
    Conduct spatial intersection using spatial index for candidates GeoDataFrame to make queries faster.
    Note, with this function, you can have multiple Polygons in the 'intersecting_gdf' and it will return all the points
    intersect with ANY of those geometries.
    """
    source_sindex = building_centroids.sindex
    possible_matches_index = []

    # 'itertuples()' function is a faster version of 'iterrows()'
    for other in buffered.itertuples():
        bounds = other.geometry.bounds
        c = list(source_sindex.intersection(bounds))
        possible_matches_index += c

    # Get unique candidates
    unique_candidate_matches = list(set(possible_matches_index))
    possible_matches = building_centroids.iloc[unique_candidate_matches]

    # Conduct the actual intersect
    result = possible_matches.loc[possible_matches.intersects(buffered.unary_union)]
    return result


# Count intersections by postal code area
buildings_cnt = gpd.sjoin(buffered, building_centroids).groupby('key').size().reset_index()

buildings_cnt = buildings_cnt.rename(columns={0: 'buildings_cnt'})
buffered['buildings'] = buffered.merge(buildings_cnt, on='key')



def apply_tariff_iterrows(building_centroids):
    energy_cost_list = []
    for index, row in building_centroids.iterrows():
        geometry = buffered['geometry'].iloc[1]
        sindex = building_centroids.sindex
        possible_matches_index = list(sindex.intersection(geometry.bounds))
        possible_matches = building_centroids.iloc[possible_matches_index]
        precise_matches = possible_matches[possible_matches.intersects(geometry)]
        energy_cost_list << precise matches

'''

#%%%
'''
###################### ELEVATION GRADES ### not sure if this is useful eventually
########## node elevations and edge grades
# add node elevations from a single raster file
raster_path = "ASTGTMV003_N28E083_dem.tif"
network_elev = ox.elevation.add_node_elevations_raster(network_proj, raster_path)
assert not np.isnan(np.array(network_elev.nodes(data="elevation"))[:, 1]).any()
# add edge grades and their absolute values
network_elev = ox.elevation.add_edge_grades(network_elev, add_absolute=True)
# plot nodes by elevation and get one color for each node, by elevation, then plot the network
nc = ox.plot.get_node_colors_by_attr(network_elev, "elevation", cmap="plasma")
fig, ax = ox.plot_graph(network_elev, node_color=nc, node_size=5, edge_color="#333333", bgcolor="k")
'''


#%%

'''
for col in edges.columns:
    if any(isinstance(val, list) for val in edges[col]):
        print('Column: {0}, has a list in it'.format(col))
        #edges['highway'] = edges['osmid'].apply(lambda x: ' '.join(x))
        #print('Column: {0}, has a list in it'.format(col))
'''


#%%
# QUESTIONS
# better to use df.loc[:,['col']] instead of df['col']?
#
#
#
join.to_file('join.shp')