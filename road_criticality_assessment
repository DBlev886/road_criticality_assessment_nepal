# -*- coding: utf-8 -*-
"""
Created on Sun May  8 12:04:30 2022

@author: Danny Baldig

Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
venv\scripts\activate


# = comments on what code does
### = methodological comment (just for me)
##### = new sub-chapter
########## = new chapter
"""


#%%
########## INPUTS and IMPORTS
download_path ='C:/Users/Danny/Desktop/EMMA/IV/Data/' # enter path for (temporary) downloads
lc_path = "C:/Users/Danny/Desktop/EMMA/IV/Data/Land_cover/data/LandCover_NP_2019.tif" # path of land cover raster; trying to automate this later but download is behind log-in wall, so that requires an account for every user
study_area = (["Beni, Nepal", "Jaljala, Nepal"])    # Use OSM Nominatim in comma-seperated strings: https://nominatim.openstreetmap.org/ui/search.html
population_type = "constrained" #("constrained": for official population counts or "UNadj_constrained" for UN adjusted population counts)


### get rid of unused packages -> linter doesn't work
import osmnx as ox
import networkx as nx
import geopandas as gpd
import rasterio as rio
import rasterstats
from rasterio.warp import calculate_default_transform, reproject, Resampling
from rasterio.plot import show_hist
import rioxarray as rxr
import pandas as pd
from shapely.ops import linemerge
import fiona
from pyproj import CRS
import numpy as np
from matplotlib import pyplot
import matplotlib.pyplot as plt
import matplotlib as mpl
from shapely import speedups
speedups.enabled
from shapely.ops import unary_union
from shapely.ops import substring
from shapely import wkt
%matplotlib inline
import pprint
from shapely.geometry import Point, Polygon, MultiPoint, MultiLineString, MultiPolygon, LineString, shape
from math import sqrt
import pycountry
from rasterio.mask import mask
import os
from wpgpDownload.utils.wpcsv import ISO_LIST
from wpgpDownload.utils.wpcsv import Product
from wpgpDownload.utils.convenience_functions import download_country_covariates as dl

#pd.set_option('display.max_colwidth', None)


#%%
########## RETRIEVE AND PROCESS OSM-DATA
# fetch OSM street network data
### use custom_filter instead of network_type for specific road types or utility lines
network = ox.graph_from_place(study_area, network_type="drive", clean_periphery=True)

# project the data
network_proj = network#ox.project_graph(network)
nodes, edges = ox.graph_to_gdfs(network_proj, nodes=True, edges=True)
#edges = edges.dropna()

# define boundaries to mask raster
### buffer distance crucial to connect roads on the outside and to allow connectivitiy between multiple administrative units
AOI = ox.geocode_to_gdf(study_area)
AOI = AOI.dissolve()
AOI = AOI.to_crs(CRS(nodes.crs))
AOI_file = download_path + "aoi.shp"
AOI.to_file(AOI_file)


#%%
'''
# plot data
fig, ax = plt.subplots(figsize=(12,8))
edges.plot(ax=ax, linewidth=0.75, color='gray')
nodes.plot(ax=ax, markersize=2, color='gray')
'''


#%%
# creating and executing function that cuts edges in 1000m (default) segments from its starting point

edges = edges.to_crs(32644)

edges.index = edges.index.get_level_values(0)
edges.columns = edges.columns.get_level_values(0)

def coord_lister(geom):
    coords = list(geom.coords)
    return (coords)
coords = edges.geometry.apply(coord_lister)

def cut(line, distance=1000):
# Cuts a line in two at a distance from its starting point
    #if distance <= 0.0 or distance >= line:
    if np.where(line.length < distance, 0, 1):
       return [LineString(line)]
    #coords = list(line.coords)
    for i, p in enumerate(coords):
        pd = line.project(Point(p))
        if pd == distance:
            return [
                    LineString(coords[:i+1]),
                    LineString(coords[i:])]
        if pd > distance:
            cp = line.interpolate(distance)
            return [
                    LineString(coords[:i] + [(cp.x, cp.y)]),
                    LineString([(cp.x, cp.y)] + coords[i:])]

edges['cut'] = edges['geometry'].apply(cut)

edges = edges.assign(new_test=np.where(edges.cut.isnull(), edges.geometry, edges.cut))

#%%
'''# geometries appear to be in the wrong format to be used
from shapely import wkt

edges['new_test'] = edges['new_test'].astype('str') 
#edges['new_test'] = edges['new_test'].astype('|S') #where the max length is set at 80 bytes,
print(edges.dtypes)
df['geometry'] = edges['new_test'].apply(wkt.loads)'''



#%%
########## ASSIGN STRATEGICAL IMPORTANCE SCORES
###### ASSIGN CENTRALITY SCORES
### normalize centrality values to summarize them with population later to create quantiles
centrality = nx.closeness_centrality(nx.line_graph(network_proj))
edges['centrality'] = pd.DataFrame.from_dict(centrality, orient='index')
edges['normalized_centrality'] = (edges['centrality']-edges['centrality'].min())/(edges['centrality'].max()-edges['centrality'].min())


#%%
##### ADD POPULATION RASTER AND ASSIGN POPULATION SCORES
# identify ISO country code to automatically download population raster based on OSM-input
if not download_path.endswith("/"):
    download_path = download_path + "/"

study_area_str = " ".join(study_area)
for country in pycountry.countries:
    if country.name in study_area_str:
        iso_code = country.alpha_3

'''products = Product(iso_code)  # Where instead of NPL it could be any valid ISO code.
#  to list all the products for NPL
for p in products:
    if "2020" in p.dataset_name: 
        print('%s/%s\t%s\t%s' % (p.idx, p.country_name,p.dataset_name,p.path))'''

prod_name_input = "ppp_2020_" + population_type
dl(ISO=iso_code, out_folder=download_path, prod_name=prod_name_input)

# open downloaded raster, reproject, and mask with AOI boundary
# ERROR even though file is not opened somewhere else: CPLE_AppDefinedError: Deleting C:/Users/Danny/Desktop/EMMA/IV/Data/Population/npl_ppp_2020_UNadj_clipped.tif failed: Permission denied
raster_crs = edges.crs
mask_coords = AOI['geometry']
file_name = iso_code + "_" + prod_name_input + ".tif"
file_name = file_name.lower()
pop_path = download_path + file_name
file_substr = ".tif"
pop_idx = pop_path.index(file_substr)
pop_proj_tif = pop_path[:pop_idx] + "_reproj" + pop_path[pop_idx:]
pop_proj_clip_tif = pop_proj_tif[:pop_idx] + "_clipped2" + pop_proj_tif[pop_idx:]

with rio.open(pop_path, mode='r+') as pop:
    transform, width, height = calculate_default_transform(
        pop.crs, raster_crs, pop.width, pop.height, *pop.bounds)
    kwargs = pop.meta.copy()
    kwargs.update({
        'crs': raster_crs,
        'transform': transform,
        'width': width,
        'height': height
    })

    with rio.open(pop_proj_tif, 'w', **kwargs) as pop_proj:
        for i in range(1, pop.count + 1):
            reproject(
                source=rio.band(pop, i),
                destination=rio.band(pop_proj, i),
                lc_transform=pop.transform,
                lc_crs=pop.crs,
                lc_proj_transform=transform,
                raster_crs=raster_crs,
                resampling=Resampling.nearest)

with rio.open(pop_proj_tif) as pop_proj:
    pop_out_image, out_transform = rio.mask.mask(pop_proj, mask_coords, crop=True)
    pop_out_meta = pop.meta

pop_out_meta.update({"driver": "GTiff",
                 "height": pop_out_image.shape[1],
                 "width": pop_out_image.shape[2],
                 "transform": out_transform})

# gives wrong CRS for some reason
with rio.open(pop_proj_clip_tif, "w", **pop_out_meta) as pop_dest:
    pop_dest.write(pop_out_image)

# read clipped population raster and assign values to numpy nd array
#pop_count =  rio.open(clipped)
pop_raster = rio.open(pop_proj_clip_tif)
pop_count_array = pop_raster.read(1)
affine = pop_raster.transform

# calculating zonal statistics
pop_mean = rasterstats.zonal_stats(edges, pop_count_array, affine = affine,
                                            stats = ['mean'],
                                            geojson_out = True)

# extract average population data from list
pop_mean_list = []
i = 0

while i < len(pop_mean):
    pop_mean_list.append(pop_mean[i]['properties'])
    i = i + 1

# transfer information from list to DataFrame and assign scores based on quantiles ## get NaN values if not converting index
pop_mean = pd.DataFrame(pop_mean_list)
pop_mean = pop_mean.set_index(edges.index)
edges['pop_mean'] = pop_mean['mean']
edges['normalized_pop'] = (edges['pop_mean']-edges['pop_mean'].min())/(edges['pop_mean'].max()-edges['pop_mean'].min())
edges['strategic_importance_quantile'] = edges['normalized_centrality'] + edges['normalized_pop']


#%%%
# qcut: quantile-based discretization to assign scores from 1 to 5 to centrality values
edges['strategic_importance_score'] = pd.qcut(edges['strategic_importance_quantile'], q=5, labels=[1,2,3,4,5]).astype(str)

edges.plot('normalized_centrality', label='centrality', cmap='RdYlBu_r', legend=True)
edges.plot('normalized_pop', label='population', cmap='RdYlBu_r', legend=True)
edges.plot('strategic_importance_score', label='strategic importance', cmap='RdYlBu_r', legend=True)


#%%
# trying to automatize land cover download /// doesn't work yet
import requests
import bs4

site_url = 'http://rds.icimod.org/DatasetMasters/DownloadFile/19?metadataid=1972729'
userid = 'danny.baldig@student.uibk.ac.at'
password = 'icimodPW977'

file_url = 'http://rds.icimod.org/DatasetMasters/DownloadFile/19?metadataid=1972729'
o_file = 'nlcms_2019.zip'

# create session
s = requests.Session()
# GET request. This will generate cookie for you
s.get(site_url)
# login to site.
s.post(site_url, data={'_username': userid, '_password': password})
# Next thing will be to visit URL for file you would like to download.
r = s.get(file_url)

# Download file
with open(o_file, 'wb') as output:
    output.write(r.content)
print(f"requests:: File {o_file} downloaded successfully!")

# Close session once all work done
s.close()

#%%%

lc_idx = lc_path.index(file_substr)
lc_proj_tif = lc_path[:lc_idx] + "_reproj" + lc_path[lc_idx:]
lc_proj_clip_tif = lc_proj_tif[:lc_idx] + "_clipped3" + lc_proj_tif[lc_idx:]

with rio.open(lc_path) as lc:
    transform, width, height = calculate_default_transform(
        lc.crs, raster_crs, lc.width, lc.height, *lc.bounds)
    kwargs = lc.meta.copy()
    kwargs.update({
        'crs': raster_crs,
        'transform': transform,
        'width': width,
        'height': height
    })

    with rio.open(lc_proj_tif, 'w', **kwargs) as lc_proj:
        for i in range(1, lc.count + 1):
            reproject(
                source=rio.band(lc, i),
                destination=rio.band(lc_proj, i),
                lc_transform=lc.transform,
                lc_crs=lc.crs,
                lc_proj_transform=transform,
                raster_crs=raster_crs,
                resampling=Resampling.nearest)

with rio.open(lc_proj_tif) as lc_proj:
    lc_out_image, out_transform = rio.mask.mask(lc_proj, mask_coords, crop=True)
    lc_out_meta = lc.meta

lc_out_meta.update({"driver": "GTiff",
                 "height": lc_out_image.shape[1],
                 "width": lc_out_image.shape[2],
                 "transform": out_transform})

# output with wrong CRS again
with rio.open(lc_proj_clip_tif, "w", **lc_out_meta) as lc_dest:
    lc_dest.write(lc_out_image)
    lc_dest.close()


#%%

# dataframe is a multiindex which causes problems
edges.index = edges.index.get_level_values(0)
edges.columns = edges.columns.get_level_values(0)

test_example = LineString(([0, 0], [2, 1], [3, 2], [3.5, 1], [5, 2]))


#line = edges["geometry"]
n = 10
def points_on_line(line):
    # or to get the distances closest to the desired one:
    # n = round(line.length / desired_distance_delta)
    distances = np.linspace(0, line.length, n)
    # or alternatively without NumPy:
    # distances = (line.length * i / (n - 1) for i in range(n))
    points = [line.interpolate(distance) for distance in distances]
    multipoint = unary_union(points)  
    new_line = LineString(points)


new = points_on_line(test_example)

#%%

# unique requires lots of processing
from rasterstats import zonal_stats

#zs = zonal_stats(edges, lc_proj_clip_tif)
edges['land_cover_majority'] = zonal_stats(edges, lc_proj_clip_tif,
            stats="majority")

edges['land_cover_count'] = zonal_stats(edges, lc_proj_clip_tif,
            stats="sada")


#test = edges['geometry'].apply(points_on_line)
#test = edges.apply(lambda x: points_on_line)

#%%

edges['land_cover_final'] = np.where(edges.land_cover_majority == edges.land_cover_minority, edges['land_cover_majority'], edges['land_cover_majority'].astype(str) + edges['land_cover_minority'].astype(str))

edges['land_cover_final'] = edges['land_cover_final'].replace(["minority: .0"],"")
edges['land_cover_final'] = edges['land_cover_final'].map(lambda x: x.lstrip('+-').rstrip('minority: .0'))

print(edges['land_cover_final'])


#%%
# Read points from shapefile
pts = nodes[['x', 'y', 'geometry']]
pts.index = range(len(pts))
coords = [(x,y) for x, y in zip(pts.x, pts.y)]

# Open the raster and store metadata
raster = rio.open(lc_proj_clip_tif)
band1 = raster.read(1)

# Sample the raster at every point location and store values in DataFrame
pts['Raster Value'] = [x[0] for x in raster.sample(coords)]


plt.imshow(band1)
plt.tight_layout()

uniq_vals = np.unique(band1)
print(sorted(uniq_vals))

counts, bins = np.histogram(band1, bins=8)
for i in range(len(bins)-1):
    print("bin lower bound:", bins[i])
    print("counts:", counts[i])

'''print(raster.mode)
print(raster.count)
print(raster.width)
print(raster.height)
print(raster.crs)
print(raster.bounds)'''

'''
##### Land cover download
https://rds.icimod.org/DatasetMasters/DownloadFile/19?metadataid=1972729
'''



#%%
###################### BUFFER EDGES
'''# buffer edges
buffered = edges.buffer(100, cap_style=2)
buffered = gpd.GeoDataFrame(geometry=gpd.GeoSeries(buffered))
#buffered = buffered.dropna(axis=0, how=\"any\", thresh=None, subset=None, inplace=False)
#buffered.plot()
'''

#%%
'''
# retrieve buildings
tags = {'building': True}
buildings = ox.geometries_from_place(study_area, tags)
buildings = buildings.to_crs(CRS(nodes.crs))


fig, ax = plt.subplots(figsize=(12,8))
# Plot edges and nodes
edges.plot(ax=ax, linewidth=0.75, color='gray')
nodes.plot(ax=ax, markersize=2, color='gray')
# Add buildings
ax = buildings.plot(ax=ax, facecolor='red', alpha=0.7)
# Add basemap
ctx.add_basemap(ax, crs=buildings.crs, source=ctx.providers.CartoDB.Positron)


# convert building polygons to points in lat, lon format and convert to gdf
building_centroids = buildings.centroid
building_centroids = gpd.GeoDataFrame(geometry=gpd.GeoSeries(building_centroids))
building_centroids = building_centroids.to_crs(nodes.crs)

building_centroids['x'] = building_centroids.geometry.apply(lambda x: x.x)
building_centroids['y'] = building_centroids.geometry.apply(lambda x: x.y)

#bounds = list(buffered.bounds.values[1])
#sindex = building_centroids.sindex


def intersect_using_spatial_index(building_centroids, buffered):
    """
    Conduct spatial intersection using spatial index for candidates GeoDataFrame to make queries faster.
    Note, with this function, you can have multiple Polygons in the 'intersecting_gdf' and it will return all the points
    intersect with ANY of those geometries.
    """
    source_sindex = building_centroids.sindex
    possible_matches_index = []

    # 'itertuples()' function is a faster version of 'iterrows()'
    for other in buffered.itertuples():
        bounds = other.geometry.bounds
        c = list(source_sindex.intersection(bounds))
        possible_matches_index += c

    # Get unique candidates
    unique_candidate_matches = list(set(possible_matches_index))
    possible_matches = building_centroids.iloc[unique_candidate_matches]

    # Conduct the actual intersect
    result = possible_matches.loc[possible_matches.intersects(buffered.unary_union)]
    return result


# Count intersections by postal code area
buildings_cnt = gpd.sjoin(buffered, building_centroids).groupby('key').size().reset_index()

buildings_cnt = buildings_cnt.rename(columns={0: 'buildings_cnt'})
buffered['buildings'] = buffered.merge(buildings_cnt, on='key')



def apply_tariff_iterrows(building_centroids):
    energy_cost_list = []
    for index, row in building_centroids.iterrows():
        geometry = buffered['geometry'].iloc[1]
        sindex = building_centroids.sindex
        possible_matches_index = list(sindex.intersection(geometry.bounds))
        possible_matches = building_centroids.iloc[possible_matches_index]
        precise_matches = possible_matches[possible_matches.intersects(geometry)]
        energy_cost_list << precise matches



###################### CENTRALITY INDICES
# node closeness centrality
node_centrality = nx.closeness_centrality(network_proj)
# plot it
df = pd.DataFrame(data=pd.Series(node_centrality).sort_values(), columns=['cc'])
df['colors'] = ox.plot.get_colors(n=len(df), cmap='inferno', start=0.2)
df = df.reindex(network_proj.nodes())
nc = df['colors'].tolist()
fig, ax = ox.plot_graph(network_proj, bgcolor='k', node_size=30, node_color=nc, node_edgecolor='none', node_zorder=2,
                        edge_color='#555555', edge_linewidth=1.5, edge_alpha=1)

# edge closeness centrality: convert graph to a line graph so edges become nodes and vice versa

# list of edge values for the orginal graph
#ev = [network_gdf['centrality'][edge + (0,)] for edge in network_proj.edges()]

# color scale converted to list of colors for graph edges
norm = colors.Normalize(vmin=min(ev)*0.8, vmax=max(ev))
cmap = cm.ScalarMappable(norm=norm, cmap=cm.inferno)
ec = [cmap.to_rgba(cl) for cl in ev]

# color the edges in the original graph with closeness centralities in the line graph
fig, ax = ox.plot_graph(network_proj, bgcolor='k', node_size=0, node_color='w', node_edgecolor='gray', node_zorder=2,
                        edge_color=ec, edge_linewidth=1.5, edge_alpha=1)


###################### ELEVATION GRADES ### not sure if this is useful eventually
########## node elevations and edge grades
# add node elevations from a single raster file
raster_path = "ASTGTMV003_N28E083_dem.tif"
network_elev = ox.elevation.add_node_elevations_raster(network_proj, raster_path)
assert not np.isnan(np.array(network_elev.nodes(data="elevation"))[:, 1]).any()
# add edge grades and their absolute values
network_elev = ox.elevation.add_edge_grades(network_elev, add_absolute=True)
# plot nodes by elevation and get one color for each node, by elevation, then plot the network
nc = ox.plot.get_node_colors_by_attr(network_elev, "elevation", cmap="plasma")
fig, ax = ox.plot_graph(network_elev, node_color=nc, node_size=5, edge_color="#333333", bgcolor="k")
'''